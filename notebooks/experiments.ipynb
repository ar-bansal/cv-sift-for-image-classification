{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a SIFT + Bag of Visual Words + Classifier pipeline, we follow these steps:\n",
    "1. Generate SIFT descriptors for each image.\n",
    "2. Cluster the SIFT descriptors to get visual vocabulary. Cluster centers are the vocabulary.\n",
    "3. Use the clustering model to predict cluster labels for each descriptor for each image. \n",
    "4. Get normalized histograms of the cluster labels for each image. This gives us a normalized count of the number of visual words that are present in the image. \n",
    "5. Use the histogram and the labels to build a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from functools import wraps\n",
    "from src.data_utils.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "\n",
    "RANDOM_STATE = int(os.environ[\"RANDOM_STATE\"])\n",
    "MLFLOW_DATA_DIR = os.environ[\"MLFLOW_DATA_DIR\"]\n",
    "MLFLOW_TRACKING_URI = os.environ[\"MLFLOW_TRACKING_URI\"]\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "\n",
    "# mlflow ui --backend-store-uri \"sqlite:///mlflow_data/mlruns.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_id(experiment_name: str):\n",
    "    \"\"\"\n",
    "    Retrieve the experiment ID for the experiment name. Create \n",
    "    a new experiment if it does not exist.\n",
    "\n",
    "    Parameters:\n",
    "        - experiment_name (str): The MLFlow experiment name.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "        experiment_id = experiment.experiment_id\n",
    "    except AttributeError:\n",
    "        artifact_location = os.path.join(\n",
    "            MLFLOW_DATA_DIR, \n",
    "            experiment_name\n",
    "        )\n",
    "        experiment_id = mlflow.create_experiment(experiment_name, artifact_location=artifact_location)\n",
    "\n",
    "    return experiment_id\n",
    "\n",
    "\n",
    "def mlflow_log_clustering(func):\n",
    "    \"\"\"\n",
    "    Decorator for logging model parameters, metrics, and the model artifact to MLflow.\n",
    "\n",
    "    Parameters: \n",
    "        - experiment_name (str): The MLFlow experiment name.\n",
    "    \"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Set the experiment\n",
    "        experiment_name = kwargs[\"experiment_name\"]\n",
    "        experiment_id = get_experiment_id(experiment_name)\n",
    "        mlflow.set_experiment(experiment_id=experiment_id)\n",
    "\n",
    "        with mlflow.start_run():\n",
    "            model, metrics = func(*args, **kwargs)\n",
    "\n",
    "            model_params = model.get_params()\n",
    "            mlflow.log_params(model_params)\n",
    "\n",
    "            params = kwargs\n",
    "            for key, value in params.items():\n",
    "                if key != \"experiment_name\":\n",
    "                    mlflow.log_param(key, value)\n",
    "\n",
    "            mlflow.log_metrics(metrics)\n",
    "\n",
    "            mlflow.sklearn.log_model(\n",
    "                model, \n",
    "                artifact_path=experiment_name, \n",
    "                serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_PICKLE\n",
    "            )\n",
    "\n",
    "        return model, metrics\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@mlflow_log_clustering\n",
    "def run_clustering_pipeline(X_train, y_train, pipeline: Pipeline, experiment_name: str):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    predicted_labels = pipeline.predict(X_train)\n",
    "    silhouette = silhouette_score(X_train, predicted_labels)\n",
    "\n",
    "    metrics = {\n",
    "        \"silhouette_score\": silhouette\n",
    "    }\n",
    "\n",
    "    return pipeline, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset(\"train\")\n",
    "train_descriptors, train_suits, train_nums = train_ds.load_descriptors(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   suits    10 non-null     object\n",
      " 1   numbers  10 non-null     object\n",
      "dtypes: object(2)\n",
      "memory usage: 288.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "# Using dataframes to leverage groupbys\n",
    "train_df = pd.DataFrame(\n",
    "    {\n",
    "        \"suits\": [arr[0, 0].astype(\"str\") for arr in train_suits], \n",
    "        \"numbers\": [arr[0, 0].astype(\"str\") for arr in train_nums]\n",
    "    }\n",
    ")\n",
    "\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling suits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "suits\n",
       "diamonds    10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"suits\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since clubs have the lowest count, we take 1806 samples from each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_per_class = 10\n",
    "train_mask = train_df.groupby(\n",
    "    \"suits\"\n",
    ").sample(\n",
    "    num_samples_per_class, \n",
    "    random_state=RANDOM_STATE\n",
    ").index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptors_filtered = [train_descriptors[i] for i in train_mask]\n",
    "train_suits_filtered = [train_suits[i] for i in train_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/02 09:50:21 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "model, metrics =  run_clustering_pipeline(\n",
    "    np.vstack(train_descriptors_filtered), \n",
    "    np.vstack(train_suits_filtered), \n",
    "    KMeans(10, random_state=RANDOM_STATE), \n",
    "    experiment_name=\"clustering\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/29 17:03:08 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "clustering_exp_id = get_experiment_id(\"clustering\")\n",
    "\n",
    "# while False:\n",
    "# for k in range(100, 501, 50):\n",
    "\n",
    "mlflow.set_experiment(experiment_id=clustering_exp_id)\n",
    "k = 10\n",
    "with mlflow.start_run() as mlflow_run:\n",
    "    kmeans_model = KMeans(\n",
    "        n_clusters=k, \n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    kmeans_model.fit(np.vstack(train_descriptors_filtered[:10]))\n",
    "\n",
    "    mlflow.sklearn.log_model(\n",
    "        kmeans_model, \n",
    "        f\"clustering/{mlflow_run.info.run_id}\", \n",
    "        serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_PICKLE\n",
    "    )\n",
    "\n",
    "    mlflow.log_params({'k': k})\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
